<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Interaction, Accessibility & UX - Computer Vision for Assistive Object-Finding tutorial.">
    <title>Interaction, Accessibility & UX - Assistive Object-Finding</title>
    <link rel="stylesheet" href="../assets/css/styles-clean.css?v=20">
  </head>
  <body>
    <header class="site-header" role="banner">
      <div class="container">
        <h1 class="site-title">Computer Vision for Assistive Object-Finding</h1>
        <p class="site-subtitle">A Survey of Mobile and Senior-Oriented Methods</p>
        <nav class="pill-nav" aria-label="Main Navigation">
          <div class="nav-wrapper">
            <a href="../index.html">Home</a>
            <a href="introduction.html">Introduction & Motivation</a>
            <a href="related-work.html">Survey of Assistive Vision Systems</a>
            <a href="classical-methods.html">Classical Detection Methods</a>
            <a href="modern-on-device.html">Modern On-Device Models</a>
            <a href="datasets-evaluation.html">Datasets & Evaluation</a>
            <a href="accessibility-privacy.html">Accessibility & UX</a>
            <a href="challenges-gaps.html">Failure Cases & Challenges</a>
            <a href="future-conclusion.html">Conclusion</a>
            <a href="quiz.html">Quiz</a>
            <a href="annotated-bibliography.html">Bibliography</a>
          </div>
        </nav>
      </div>
    </header>
    <main id="main" class="container" role="main" style="padding: 2rem 0;">
      <h1>Interaction, Accessibility & UX</h1>

      <!-- Reading Time -->
      <div style="text-align: center; margin: 1rem 0;">
        <span style="display: inline-block; background: rgba(0, 121, 107, 0.1); color: var(--accent); padding: 0.5rem 1rem; border-radius: 2rem; font-size: 0.9rem; font-weight: 500;">
          ‚è±Ô∏è 1.5 minutes
        </span>
      </div>

      <section aria-label="Page audio" style="margin: 1.5rem 0;">
        <audio controls preload="none" aria-label="Audio narration for Interaction, Accessibility & UX" style="width: 100%; max-width: 600px;">
          <source src="../assets/audio/accessibility-privacy.mp3" type="audio/mp4">
          Your browser does not support the audio element.
        </audio>
      </section>

      <p>
        Designing an effective assistive object-finding system is not just about detection accuracy - it's about delivering the result in a way the user can actually understand, trust, and act on. For older adults or individuals with cognitive or visual impairments, this means building systems with thoughtful interaction flows, multimodal feedback, and privacy-aware operation.
      </p>

      <h2>Guidance Modalities</h2>

      <figure style="display: flex; flex-direction: column; align-items: center; margin: 2rem auto;">
        <img src="../assets/img/guidance-modalities.png"
             alt="Icons showing audio, haptic, and visual feedback methods"
             style="max-width: 100%; height: auto;" loading="lazy" />
        <figcaption style="text-align: center; color: #64748b; font-size: 0.95rem;">
          Three types of user guidance: audio feedback (üéß), haptic vibration (üì≥), and visual prompts (üëÅ). (Original diagram created by author, based on concepts from [2].)
        </figcaption>
      </figure>

      <p>
        Once an object is detected, the system must guide the user to it. Common feedback methods include:
      </p>
      <ul>
        <li><strong>Audio cues:</strong> spoken directions ("Move left"), beeps that increase with proximity, or 3D audio spatialization</li>
        <li><strong>Haptic feedback:</strong> vibration patterns that change based on orientation or distance to target</li>
        <li><strong>Visual prompts:</strong> on-screen arrows, bounding boxes, or edge highlights to point toward the object</li>
      </ul>

      <p>
        For example, <a href="annotated-bibliography.html#ref-2" target="_blank">[2]</a> uses directional audio that updates as the user moves their phone. Participants reported that the spoken feedback made the system feel more trustworthy, especially when detection confidence was low. This shows that UX can amplify (or mitigate) the underlying model quality.
      </p>

      <h2>Accessibility Considerations</h2>
      <p>
        Designing for seniors or individuals with impairments introduces unique constraints:
      </p>
      <ul>
        <li><strong>Font size:</strong> Interfaces should default to large text and offer adjustable contrast and brightness</li>
        <li><strong>Touch targets:</strong> Buttons should be large and well-spaced to accommodate motor limitations</li>
        <li><strong>Minimal required input:</strong> Systems should avoid requiring the user to type or re-train frequently</li>
        <li><strong>Consistency:</strong> System behavior should be predictable across sessions</li>
      </ul>

      <p>
        In <a href="annotated-bibliography.html#ref-3" target="_blank">[3]</a>, the authors implemented on-screen directional prompts and found that older participants preferred visual feedback when navigating familiar indoor spaces, especially when audio wasn't practical.
      </p>

      <h2>Privacy & On-Device Processing</h2>
      <p>
        Vision-based systems inherently process user surroundings - including potentially sensitive environments like homes, bathrooms, and bedrooms. For this reason, on-device inference is preferred over cloud-based pipelines in assistive contexts.
      </p>

      <p>
        Running models locally provides several benefits:
      </p>
      <ul>
        <li>No video is transmitted or stored externally</li>
        <li>Lower latency and better responsiveness</li>
        <li>System remains usable without internet access</li>
      </ul>

      <p>
        However, local processing limits model size and complexity. This trade-off was discussed in <a href="annotated-bibliography.html#ref-5" target="_blank">[5]</a>, where the authors acknowledged that while open-vocabulary detection is powerful, it must be simplified or compressed for real-time use on edge devices.
      </p>

      <p>
        Some hybrid approaches like those in <a href="annotated-bibliography.html#ref-4" target="_blank">[4]</a> combine lightweight local detection with periodic cloud-assisted refinement, enabling a better balance between performance and privacy.
      </p>

      <p>
        In the next section, we'll explore the practical and technical challenges that still make object-finding a difficult task - including clutter, occlusion, and personalization.
      </p>

      <div style="display: flex; justify-content: space-between; margin-top: 3rem;">
        <a href="datasets-evaluation.html" class="btn" style="text-decoration: none;">‚Üê Previous</a>
        <a href="challenges-gaps.html" class="btn btn-primary" style="text-decoration: none;">Next ‚Üí</a>
      </div>
    </main>
    <script src="../assets/js/main.js?v=7" defer></script>
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        const current = location.pathname.split("/").pop() || "index.html";
        const links = document.querySelectorAll(".pill-nav a");

        links.forEach(link => {
          const href = link.getAttribute("href");
          if (href === current || href === "./" + current) {
            link.classList.add("active");
            link.setAttribute("aria-current", "page");

            // Scroll it into view
            link.scrollIntoView({ inline: "center", behavior: "smooth", block: "nearest" });
          }
        });
      });
    </script>
  </body>
</html>
